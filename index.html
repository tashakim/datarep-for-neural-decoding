<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Representations for Brain Data - ICML 2026 Workshop</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <nav>
            <div class="nav-container">
                <h1 class="logo">Neural Representations for Brain Data</h1>
                <ul class="nav-links">
                    <li><a href="#schedule">Schedule</a></li>
                    <li><a href="#speakers">Speakers</a></li>
                    <li><a href="#cfp">Call for Papers</a></li>
                    <li><a href="#organizers">Organizers</a></li>
                </ul>
            </div>
        </nav>
    </header>

    <div class="banner">
        <img src="assets/cover-1.png" alt="Workshop Banner">
    </div>

    <main>
        <section class="hero">
            <div class="container">
                <h1>ICML 2026 WORKSHOP</h1>
                <h2>Neural Representations for Brain Data</h2>
                <p class="date-location">Date: TBA<br>Location: Seoul, South Korea</p>
            </div>
        </section>

        <section class="intro-section">
            <div class="container">
                <p>
                    Brain–computer interfaces (BCIs) are advancing rapidly: while invasive systems already benefit from well-established neural representations, non-invasive modalities (e.g., EEG/MEG) are reaching the scale needed for systematic benchmarking. Yet the field still lacks consensus on how brain signals should be represented for machine learning, representing a core limitation for generalization, robustness, and data efficiency.
                </p>
                <p>
                    Unlike speech or vision, which converged on standardized inputs such as mel spectrograms, BCI research remains fragmented, often defaulting to raw or minimally processed signals despite the complexity and noise of neural data. This workshop focuses on a central question: <strong>What should representation learning and feature engineering look like for brain data in the era of deep learning?</strong>
                </p>
                <p>
                    We invite ML researchers, neuroscientists, and BCI practitioners to benchmark representations across modalities, compare engineered and learned features, and assess whether the field needs universal or task-specific encodings. By extracting the principles that govern effective neural representations, the workshop aims to build a coherent framework for modeling brain signals, directly aligned with ICML's core interest in advancing machine learning. Our long-term goal is to catalyze BCIs' "mel-spectrogram moment": determining whether the field can converge on shared representations or requires task-specific schemes. By creating shared evaluation infrastructure and design principles, the workshop aims to accelerate progress toward robust, generalizable, and deployable brain-AI systems.
                </p>
            </div>
        </section>

        <section id="schedule" class="section">
            <div class="container">
                <h2>Tentative Schedule</h2>
                <div class="schedule-table">
                    <table>
                        <thead>
                            <tr>
                                <th>Time</th>
                                <th>Session</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>09:00–09:10</td>
                                <td>Opening Remarks</td>
                            </tr>
                            <tr>
                                <td>09:10–10:00</td>
                                <td>Talk 1: Mirco Ravanelli (Concordia University/Mila)</td>
                            </tr>
                            <tr>
                                <td>10:00–10:50</td>
                                <td>Talk 2: Alexandre Gramfort (Meta Reality Labs)</td>
                            </tr>
                            <tr>
                                <td>10:50–11:10</td>
                                <td>Coffee Break & Networking</td>
                            </tr>
                            <tr>
                                <td>11:10–12:00</td>
                                <td>Talk 3: Natalie Voets (University of Oxford)</td>
                            </tr>
                            <tr>
                                <td>12:00–12:30</td>
                                <td>Spotlight Session 1: Selected papers (10 min each)</td>
                            </tr>
                            <tr>
                                <td>12:30–13:00</td>
                                <td>Morning Panel with Ravanelli, Gramfort, Voets, spotlight presenters</td>
                            </tr>
                            <tr>
                                <td>13:00–14:00</td>
                                <td>Lunch, Poster, and BCI Demo & Neural Art Sessions</td>
                            </tr>
                            <tr>
                                <td>14:00–14:50</td>
                                <td>Talk 4: SueYeon Chung (Harvard University)</td>
                            </tr>
                            <tr>
                                <td>14:50–15:40</td>
                                <td>Talk 5: Juan Helen Zhou (National University of Singapore)</td>
                            </tr>
                            <tr>
                                <td>15:40–16:00</td>
                                <td>Afternoon Coffee Break & Networking</td>
                            </tr>
                            <tr>
                                <td>16:00–16:30</td>
                                <td>Spotlight Session 2: Selected papers (10 min each)</td>
                            </tr>
                            <tr>
                                <td>16:30–17:15</td>
                                <td>Panel Discussion with all speakers</td>
                            </tr>
                            <tr>
                                <td>17:15+</td>
                                <td>Workshop Social</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </section>

        <section id="speakers" class="section">
            <div class="container">
                <h2>Invited Speakers</h2>
                <div class="people-grid">
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/mirco.jpg" alt="Mirco Ravanelli">
                        </div>
                        <h3>Mirco Ravanelli</h3>
                        <p>Concordia University, Université de Montréal, Mila Quebec AI Institute</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/natalie.webp" alt="Natalie Voets">
                        </div>
                        <h3>Natalie Voets</h3>
                        <p>University of Oxford</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/alexander.jpg" alt="Alexandre Gramfort">
                        </div>
                        <h3>Alexandre Gramfort</h3>
                        <p>Meta Reality Labs</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/sueyeon.jpg" alt="SueYeon Chung">
                        </div>
                        <h3>SueYeon Chung</h3>
                        <p>Harvard University</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/helen.jpg" alt="Juan Helen Zhou">
                        </div>
                        <h3>Juan Helen Zhou</h3>
                        <p>National University of Singapore</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/yukiyasu.webp" alt="Yukiyasu Kamitani">
                        </div>
                        <h3>Yukiyasu Kamitani</h3>
                        <p>Kyoto University, ATR Computational Neuroscience Laboratories</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="cfp" class="section">
            <div class="container">
                <h2>Call For Papers</h2>
                <p>
                    We invite researchers at the intersection of machine learning, neuroscience, and related fields to
                    submit their recent work on neural representations for BCIs. Accepted papers will be presented
                    as spotlight talks or posters during the workshop.
                </p>

                <h3>Topics of Interest</h3>
                <p>Topics of interest include, but are not limited to:</p>
                <ul>
                    <li><strong>Time–frequency–phase representations</strong> for EEG/MEG/ECoG</li>
                    <li><strong>Spatial architectures</strong> for multi-channel neural signals: CNNs, graph neural networks, or attention mechanisms</li>
                    <li><strong>Cross-spectral features</strong> (Riemannian geometry on covariance matrices)</li>
                    <li><strong>Learned vs. engineered features</strong> for neural decoding: identifying when each approach excels and why</li>
                    <li><strong>Foundation models</strong> and multi-task representations across speech/motor BCIs</li>
                    <li><strong>Universal vs. task-specific neural representations</strong>: can one representation serve all BCI applications?</li>
                </ul>

                <h3>Submission Tracks</h3>
                <p>We offer two submission tracks:</p>
                <ul>
                    <li>
                        <strong>Full Papers:</strong> 4–8 pages (excluding references and appendices), with one additional page permitted upon acceptance.
                    </li>
                    <li>
                        <strong>Short Papers:</strong> Up to 4 pages at submission time (excluding references and appendices). Accepted short papers may add one page for the camera-ready version.
                    </li>
                </ul>

                <p>References and appendices do not count toward the page limit. All submissions must use the <strong>ICML 2026</strong> style. Accepted papers may add <strong>one extra page</strong> for the camera-ready version.</p>

                <h3>Reviewing Process</h3>
                <p>
                    Submissions will undergo <strong>double-blind review</strong> on <strong>OpenReview</strong>.
                    Each paper will receive <strong>at least two reviews</strong> evaluating novelty, clarity, technical soundness, and relevance,
                    with reviewers encouraged to provide constructive, actionable feedback.
                    All conflicts of interest will be handled by the program chairs to ensure a fair and unbiased review process.
                </p>
                <p>
                    Papers will be selected for <strong>oral (spotlight)</strong> or <strong>poster</strong> presentation based on quality, originality,
                    and potential to stimulate discussion, with an emphasis on assembling a diverse and balanced program.
                    Authors of accepted papers will submit a camera-ready version incorporating reviewer feedback.
                    All accepted papers will be publicly available on OpenReview before the workshop.
                </p>

                <h3>Timeline</h3>
                <p><strong>All deadlines are 11:59pm AoE (Anywhere on Earth).</strong></p>
                <ul>
                    <li><strong>Submission opens:</strong> 9 April 2026</li>
                    <li><strong>Submission deadline (both tracks):</strong> 13 May 2026</li>
                    <li><strong>Review period:</strong> 13–26 May 2026</li>
                    <li><strong>Acceptance notification:</strong> 2 June 2026</li>
                    <li><strong>Camera-ready deadline:</strong> 16 June 2026</li>
                </ul>

                <p>
                    Accepted papers will be presented during poster sessions, with exceptional submissions selected for spotlight oral presentations.
                    All accepted papers will be made publicly available as <strong>non-archival</strong> reports, allowing for future submissions to archival conferences or journals.
                </p>
            </div>
        </section>

        <section id="demos" class="section">
            <div class="container">
                <h2>Demos & Neural Art Track</h2>
                <p>
                    In addition to research presentations, we introduce a Demos & Neural Art track connecting neural representations with generative models driven directly by brain activity. This track features systems from both academia and industry, including Prof. Yukiyasu Kamitani on visual imagery decoding, the OBVIOUS collective (IMAGINE mind-to-image series), and Blackrock Neurotech's brain-controlled art and music.
                </p>
                <div class="demo-images">
                    <div class="demo-image-item">
                        <div class="demo-image-wrapper">
                            <img src="assets/kamitani.jpeg" alt="Visual Imagery Decoding (Kamitani Lab, 2025)">
                            <div class="image-overlay">
                                <p><strong>Visual imagery reconstruction from fMRI brain activity.</strong><br><br>
                                    Panel (c) shows imagined shapes and objects (top) decoded into visual reconstructions (bottom) using hierarchical DNN feature representations, 
                                    demonstrating that perception-trained models can decode mental imagery. 
                                    Panels (d-e) illustrate how attention modulates reconstruction quality: attending to specific stimulus features enhances their neural representation, 
                                    enabling selective decoding of attended content. From Kamitani Lab's Annual Review of Vision Science (2025).
                                    <br><br>Image: Yukiyasu Kamitani / Kamitani Lab, Kyoto University. Published in Annual Review of Vision Science (2025), DOI: 10.1146/annurev-vision-110423-023616
                                </p>
                            </div>
                        </div>
                        <p class="image-caption">Visual Imagery Decoding (Kamitani Lab, 2025)</p>
                    </div>

                   <div class="demo-image-item">
                        <div class="demo-image-wrapper">
                            <img src="assets/obvious.png" alt="The Anger Falls Silent by OBVIOUS">
                            <div class="image-overlay">
                                <p><strong>"The Anger Falls Silent"</strong> (2024) by OBVIOUS<br><br>
                                Created from brain activity recorded inside an MRI scanner, while the artist imagined a volcano.<br><br>
                                <em>"She cascades from the hollow of a volcano, her thoughts embrace the skies, and her fire sweeps across the earth, nourishing its vessels until weariness is found, and sometimes the anger falls silent."</em><br><br>
                                
                                Two-layer digigraphic print on cotton texture paper, 4-Blacks deep overprint, signed with GAN model loss function in ink. Dimensions: 164×135 cm (framed), 146×114 cm (unframed)<br><br>
                                </p>
                            </div>
                        </div>
                        <p class="image-caption"><em>"The Anger Falls Silent"</em> (OBVIOUS, 2024)</p>. 
                    </div>

                    <div class="demo-image-item">
                        <div class="demo-image-wrapper">
                            <div class="video-container">
                                <iframe src="https://www.youtube.com/embed/eW0jn7jAe1w" 
                                        title="Blackrock Neurotech Brain-Controlled Art" 
                                        frameborder="0" 
                                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                                        allowfullscreen>
                                </iframe>
                            </div>
                            <div class="image-overlay">
                                <p><strong>Brain-controlled art by Blackrock Neurotech</strong><br><br>
                               James Johnson, paralyzed from a spinal cord injury, creates digital art using Blackrock Neurotech's Utah Array brain implant. This demonstration shows real-time thought-to-cursor control in Adobe Photoshop, enabling creative expression through neural signals alone.<br><br>Video: Blackrock Neurotech | Featuring James Johnson with research by CalTech (Tyson Aflalo, Richard Andersen) </div>
                        </div>
                        <p class="image-caption">Brain-Controlled Art (Blackrock Neurotech, 2023)</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="organizers" class="section">
            <div class="container">
                <h2>Organizers</h2>
                <p class="organizer-email"><strong>Contact Email:</strong> <a href="mailto:TBA">TBA</a></p>
                <div class="people-grid">
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/mariya.png" alt="Mariya Hendriksen">
                        </div>
                        <h3>Mariya Hendriksen</h3>
                        <p>University of Oxford</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/tasha.png" alt="Tasha Kim">
                        </div>
                        <h3>Tasha Kim</h3>
                        <p>University of Oxford</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/ninon.png" alt="Ninon Lizé Masclef">
                        </div>
                        <h3>Ninon Lizé Masclef</h3>
                        <p>Massachusetts Institute of Technology</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/teyun.png" alt="Teyun Kwon">
                        </div>
                        <h3>Teyun Kwon</h3>
                        <p>University of Oxford</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/francesco.png" alt="Francesco Mantegna" onerror="this.style.display='none'; this.parentElement.classList.add('person-photo-placeholder');">
                        </div>
                        <h3>Francesco Mantegna</h3>
                        <p>University of Oxford</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/margaret.png" alt="Margaret Henderson">
                        </div>
                        <h3>Margaret Henderson</h3>
                        <p>Carnegie Mellon University</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/philip.png" alt="Philip Torr">
                        </div>
                        <h3>Philip Torr</h3>
                        <p>University of Oxford</p>
                    </div>
                    <div class="person">
                        <div class="person-photo">
                            <img src="assets/oiwi.png" alt="Oiwi Parker Jones">
                        </div>
                        <h3>Oiwi Parker Jones</h3>
                        <p>University of Oxford</p>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2026 Neural Representations for Brain Data Workshop. Part of ICML 2026.</p>
        </div>
    </footer>
</body>
</html>
